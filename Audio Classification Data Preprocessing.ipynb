{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e761ee7f",
   "metadata": {},
   "source": [
    "# Audio Classification Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's read a sample audio using librosa\n",
    "import librosa\n",
    "audio_file_path='UrbanSound8K/100263-2-0-3.wav'\n",
    "librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1452340",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets plot the librosa audio data\n",
    "import matplotlib.pyplot as plt\n",
    "# Original audio with 1 channel \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c079981",
   "metadata": {},
   "source": [
    "# Observation\n",
    "Here Librosa converts the signal to mono, meaning the channel will alays be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets read with scipy\n",
    "from scipy.io import wavfile as wav\n",
    "wave_sample_rate, wave_audio = wav.read(audio_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40af5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original audio with 2 channels \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(wave_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54877935",
   "metadata": {},
   "source": [
    "# Extract Features\n",
    "Here we will be using Mel-Frequency Cepstral Coefficients(MFCC) from the audio samples. The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .)The blw is used to extract the features of 1 audio file i audio.\n",
    "mfccs = librosa.feature.mfcc(y=librosa_audio_data,sr=librosa_sample_rate,n_mfcc=40)\n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73408673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extracting MFCC's For every audio file\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "audio_dataset_path='UrbanSound8K/audio/'\n",
    "metadata=pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28661257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making fun to  extract features of all 8000 audios in folder.\n",
    "def features_extractor(file):\n",
    "    audio,sample_rate = librosa.load(file_name,res_type='kaiser_fast')\n",
    "    mfccs_features = librosa.features.mfcc(y=audio,sr=sample_rate,n_mffc=40)\n",
    "#     .)the blw will return the scales features of each audio.\n",
    "    mfccs_scaled_features = np.mean(mfccs.T,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31460afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .)Now i will iterate to all audios to aextract the featured fom audio by \n",
    "# iterating the rows of dataframe.\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name= os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row['fold']),\n",
    "                           '/',str(row['slice_file_name']))\n",
    "    final_class_labels = row['class']\n",
    "    data = features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fca29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf963c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding\n",
    "###y=np.array(pd.get_dummies(y))\n",
    "### Label Encoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# .The LabelEncoder from scikit-learn is used to convert the categorical labels in y\n",
    "# into numerical labels. For example, if you have classes 'cat', 'dog', and 'bird',\n",
    "# the label encoder will map them to integers, such as 0, 1, and 2.\n",
    "labelencoder=LabelEncoder()\n",
    "\n",
    "# The to_categorical function from Keras is then used to convert the numerical\n",
    "# labels obtained from the label encoder into one-hot encoded vectors.\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3933da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea83d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a60fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa705d9",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a668a",
   "metadata": {},
   "outputs": [],
   "source": [
    " ### No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49bdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trianing my model\n",
    "\n",
    "# . The ModelCheckpoint callback is a useful tool that allows you to save the model's\n",
    "# weights during training, typically based on a certain criterion such as the \n",
    "# validation accuracy.\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
    "          validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"UrbanSound8K/dog_bark.wav\"\n",
    "prediction_feature=features_extractor(filename)\n",
    "\n",
    "# .)the blw (1,-1) This reshaping is common when you want to convert a feature vector\n",
    "# into a format that can be fed into a machine learning model. Many machine learning \n",
    "# models, especially those from libraries like scikit-learn or Keras, expect input data\n",
    "# to be in a certain shape. Reshaping is also commonly used when you want to flatten \n",
    "# an array or when dealing with images represented as multi-dimensional arrays.\n",
    "\n",
    "prediction_feature=prediction_feature.reshape(1,-1)\n",
    "model.predict_classes(prediction_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc967afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff2f29",
   "metadata": {},
   "source": [
    "# Testing Some Test Audio Data\n",
    "Steps\n",
    "\n",
    "Preprocess the new audio data\n",
    "\n",
    "\n",
    "predict the classes\n",
    "\n",
    "\n",
    "Invere transform your Predicted Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36606a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"UrbanSound8K/drilling_1.wav\"\n",
    "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "print(mfccs_scaled_features)\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "print(mfccs_scaled_features)\n",
    "print(mfccs_scaled_features.shape)\n",
    "predicted_label=model.predict_classes(mfccs_scaled_features)\n",
    "print(predicted_label)\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "prediction_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
